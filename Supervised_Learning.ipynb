{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Supervised Learning: Regression Models and Performance Metrics |**"
      ],
      "metadata": {
        "id": "vnjH9Xw9cOLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 : What is Simple Linear Regression (SLR)? Explain its purpose.**\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "**1. Definition:**\n",
        "\n",
        "  * Simple Linear Regression (SLR) is a statistical method used to analyze the relationship between two variables:\n",
        "\n",
        "  * Independent variable (X): The variable that is used to predict or explain the outcome.\n",
        "\n",
        "  * Dependent variable (Y): The variable whose value is being predicted.\n",
        "\n",
        "  * SLR assumes that the relationship between X and Y can be approximated using a straight line (linear relationship).\n",
        "\n",
        "**2. Mathematical Representation:**\n",
        "\n",
        "## Y=β0​+β1​X+ϵ\n",
        "\n",
        "  Where:\n",
        "\n",
        "  * Y = Dependent variable (outcome)\n",
        "  * X = Independent variable (predictor)\n",
        "  * β0 = Intercept (value of Y when X = 0)\n",
        "  * β1 = Slope (change in Y for a one-unit change in X)\n",
        "  * ϵ = Random error term\n",
        "\n",
        "  The slope ( β 1 ) shows the direction and strength of the relationship:\n",
        "  * Positive slope → Y increases as X increases.\n",
        "  * Negative slope → Y decreases as X increases.\n",
        "\n",
        "**3. Assumptions of SLR:**\n",
        "\n",
        "  * For SLR to give valid results, the following assumptions should hold:\n",
        "\n",
        "  * Linearity: The relationship between X and Y is linear.\n",
        "\n",
        "  * Independence: Observations are independent of each other.\n",
        "\n",
        "  * Homoscedasticity: Constant variance of errors (ϵ) across all values of X.\n",
        "\n",
        "  * Normality of errors: The errors (ϵ) are normally distributed.\n",
        "\n",
        "  * No multicollinearity: Since SLR has only one predictor, this is automatically satisfied.\n",
        "\n",
        "**4. Purpose of Simple Linear Regression:**\n",
        "\n",
        "  * Prediction: Estimate or predict the dependent variable (Y) for a given value of the independent variable (X).\n",
        "\n",
        "  * Understanding Relationships: Determine how strongly X influences Y, and whether the relationship is positive (direct) or negative (inverse).\n",
        "\n",
        "  * Trend Analysis: Identify patterns in data, which can be used for decision-making or forecasting.\n",
        "\n",
        "  * Decision Support: Helps in making business, scientific, or economic decisions based on predicted outcomes.\n",
        "\n",
        "**5. Example:**\n",
        "\n",
        "  * Suppose we want to predict a student’s exam score (Y) based on hours studied (X). If the regression equation is:\n",
        "\n",
        "## Y=50+5X\n",
        "\n",
        "\n",
        "  * The intercept (50) indicates the score if the student studies 0 hours.\n",
        "\n",
        "  * The slope (5) means that for every additional hour studied, the score increases by 5 marks.\n",
        "\n",
        "  * Using this equation, we can predict scores for different study hours.\n",
        "\n",
        "**6. Conclusion:**\n",
        "\n",
        "  * SLR is a simple but powerful tool for predicting outcomes, understanding variable relationships, and identifying trends in data. It forms the foundation for more advanced regression techniques used in statistics, data science, and machine learning.\n",
        "\n",
        "---------------------------\n",
        "**Question 2: What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Simple Linear Regression (SLR) is a widely used statistical technique, but for it to produce reliable and valid results, certain assumptions must be satisfied. These assumptions ensure that the regression model is accurate, unbiased, and interpretable. The key assumptions are as follows:\n",
        "\n",
        "**1. Linearity**\n",
        "\n",
        "  * The relationship between the independent variable (X) and the dependent variable (Y) must be linear.\n",
        "\n",
        "  * This means that changes in X are associated with proportional changes in Y.\n",
        "\n",
        "  * Reason: If the relationship is not linear, a straight-line regression model will not fit the data well, leading to incorrect predictions.\n",
        "\n",
        "  * Example: Predicting sales (Y) from advertising spend (X) should show a roughly straight-line pattern on a scatter plot.\n",
        "\n",
        "**2. Independence of Errors (No Autocorrelation)**\n",
        "\n",
        "  * The residuals (errors) should be independent of each other.\n",
        "\n",
        "  * Reason: If errors are correlated (common in time series data), standard errors may be underestimated, leading to false significance.\n",
        "\n",
        "  * Example: Daily temperature readings might have correlated errors, violating this assumption.\n",
        "\n",
        "**3. Homoscedasticity (Constant Variance of Errors)**\n",
        "\n",
        "  * The variance of the errors (ϵ) should be constant across all levels of X.\n",
        "\n",
        "  * Reason: If the variance of errors changes (heteroscedasticity), the regression coefficients may be inefficient, and hypothesis tests may be invalid.\n",
        "\n",
        "  * Check: Plot residuals vs. predicted values; the spread should be uniform.\n",
        "\n",
        "**4. Normality of Errors**\n",
        "\n",
        "  * The residuals should be normally distributed for valid confidence intervals and hypothesis testing.\n",
        "\n",
        "  * Reason: This assumption is especially important when making inferences about regression coefficients or predicting future values.\n",
        "\n",
        "  * Check: Use a histogram or Q-Q plot of residuals.\n",
        "\n",
        "**5. No Multicollinearity**\n",
        "\n",
        "  * In Simple Linear Regression, this is automatically satisfied because there is only one independent variable.\n",
        "\n",
        "  * Reason: Multicollinearity (high correlation between independent variables) affects multiple regression models but is not a concern for SLR.\n",
        "\n",
        "**6. No Measurement Error in Independent Variable**\n",
        "\n",
        "  * The independent variable (X) should be measured accurately without errors.\n",
        "\n",
        "  * Reason: Errors in X can lead to biased estimates of the slope (β1).\n",
        "\n",
        "**Summary Table of Key Assumptions**\n",
        "\n",
        "| Assumption                | Description                                   |\n",
        "| ------------------------- | --------------------------------------------- |\n",
        "| Linearity                 | Relationship between X and Y is linear        |\n",
        "| Independence of Errors    | Errors are independent of each other          |\n",
        "| Homoscedasticity          | Constant variance of errors across X          |\n",
        "| Normality of Errors       | Residuals follow a normal distribution        |\n",
        "| No Multicollinearity      | Only one X in SLR, so automatically satisfied |\n",
        "| No Measurement Error in X | X is measured accurately                      |\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "  * These assumptions are crucial for the reliability and validity of the SLR model. If any assumption is violated, it may lead to biased coefficients, incorrect predictions, or misleading inferences. Therefore, before using a regression model, analysts often check and validate these assumptions using graphical and statistical methods.\n",
        "\n",
        "----------------------------\n",
        "\n",
        "**Question 3: Write the mathematical equation for a simple linear regression model and explain each term.**\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "**1. Mathematical Equation of Simple Linear Regression (SLR)**\n",
        "\n",
        "The general form of the Simple Linear Regression equation is:\n",
        "\n",
        "  ## Y=β0​+β1​X+ϵ\n",
        "\n",
        "Where:\n",
        "\n",
        "* (Y) = Dependent variable (the outcome we want to predict)\n",
        "* (X) = Independent variable (the predictor variable)\n",
        "* β0 = Intercept (value of Y when X = 0)\n",
        "* β1 = Slope (change in Y corresponding to a one-unit change in X)\n",
        "* ϵ = Error term (the difference between observed and predicted values of Y)\n",
        "\n",
        "This equation represents a straight line that best fits the data points in a two-dimensional space.\n",
        "\n",
        "**2. Detailed Explanation of Each Term**\n",
        "\n",
        "**a) Dependent Variable ((Y))**\n",
        "\n",
        "* Also called the response variable.\n",
        "* Its value depends on X and random factors captured by **ϵ**.\n",
        "* It is the variable we are trying to predict or model.\n",
        "* **Example:** Exam score, house price, sales revenue.\n",
        "\n",
        "**b) Independent Variable ((X))**\n",
        "\n",
        "* Also called the predictor or explanatory variable.\n",
        "* It is used to explain or predict the dependent variable.\n",
        "* Only one X is used in SLR.\n",
        "* **Example:** Hours studied, size of a house, advertising budget.\n",
        "\n",
        "**c) Intercept β0**\n",
        "\n",
        "* The expected value of Y when (X = 0).\n",
        "* Graphically, it is where the regression line crosses the Y-axis.\n",
        "* **Interpretation:** Indicates the baseline level of Y when the predictor has no effect.\n",
        "* **Example:** If β0 = 40, a student who studies 0 hours is predicted to score 40.\n",
        "\n",
        "**d) Slope β1**\n",
        "\n",
        "* Represents the rate of change of Y for a unit change in X.\n",
        "* **Sign of the slope:**\n",
        "\n",
        "  * Positive (β1 > 0) → Y increases as X increases\n",
        "  * Negative (β1 < 0) → Y decreases as X increases\n",
        "* **Calculation formula:**\n",
        "\n",
        "  ## β1​= (Xi​−Xˉ)(Yi​−Yˉ) / ∑(Xi​−Xˉ)2∑\n",
        "\n",
        "* **Interpretation:** Shows the strength and direction of the relationship.\n",
        "* **Example:** If β1 = 6, each extra hour of study increases the predicted score by 6 marks.\n",
        "\n",
        "**e) Error Term ϵ**\n",
        "\n",
        "* Captures random variation or noise in Y that is not explained by X.\n",
        "* Assumed to have a mean of **0**, constant variance, and to be normally distributed.\n",
        "* Purpose: Ensures the regression line does not perfectly fit the data because real-world data is never perfect.\n",
        "\n",
        "**3. Assumptions Related to the Regression Equation**\n",
        "\n",
        "1. The relationship between X and Y is linear.\n",
        "2. Residuals **ϵ** are independent.\n",
        "3. Residuals have constant variance (homoscedasticity).\n",
        "4. Residuals are normally distributed for inference.\n",
        "5. X is measured without error.\n",
        "\n",
        "These assumptions are crucial to ensure the coefficients **(β0,β1)** are unbiased and efficient.\n",
        "\n",
        "**4. Graphical Interpretation**\n",
        "\n",
        "* Regression line: The straight line **Y=β0​+β1​X** passes through the center of data points.\n",
        "* Residuals: Vertical distances between actual points and the regression line represent (\\epsilon).\n",
        "* The slope indicates steepness and direction.\n",
        "* The intercept shows the starting point on Y-axis.\n",
        "\n",
        "**5. Conclusion**\n",
        "\n",
        "The SLR equation **Y=β0​+β1​X** provides a complete framework for:\n",
        "\n",
        "* Prediction (estimating Y for any X)\n",
        "* Understanding relationships (how X affects Y)\n",
        "* Data analysis and decision-making\n",
        "  Each term in the equation has a clear interpretation, making SLR an essential tool in statistics, economics, education, and business.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 4: Provide a real-world example where simple linear regression can be applied.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Simple Linear Regression (SLR) can be applied in many real-world scenarios where there is a relationship between two variables, and one variable can be used to predict the other.\n",
        "\n",
        "**Example: Predicting House Prices Based on Size**\n",
        "\n",
        "**Scenario:**\n",
        "A real estate company wants to predict the price of a house (Y) based on its size in square feet (X).\n",
        "\n",
        "* **Dependent Variable (Y):** House price in dollars\n",
        "* **Independent Variable (X):** House size in square feet\n",
        "\n",
        "The company collects data for several houses:\n",
        "\n",
        "| House Size (sq ft) | Price ($) |\n",
        "| ------------------ | --------- |\n",
        "| 1000               | 150,000   |\n",
        "| 1200               | 180,000   |\n",
        "| 1500               | 210,000   |\n",
        "| 1800               | 240,000   |\n",
        "| 2000               | 270,000   |\n",
        "\n",
        "**Step 1: Visualize the Relationship**\n",
        "\n",
        "* Plotting a scatter diagram shows that as house size increases, price increases.\n",
        "* This suggests a positive linear relationship.\n",
        "\n",
        "**Step 2: Formulate the Regression Equation**\n",
        "\n",
        "The SLR model is:\n",
        "\n",
        "## Y=β0​+β1​X+ϵ\n",
        "\n",
        "* Using the data, the regression coefficients can be calculated (manually or using software).\n",
        "\n",
        "* Suppose calculations give:\n",
        "\n",
        "## β0​=30,000,       β1​=120\n",
        "\n",
        "* The regression equation becomes:\n",
        "  \n",
        "##  text = 30,000 + 120.(Size in sq ft)\n",
        "\n",
        "**Step 3: Interpret the Equation**\n",
        "\n",
        "* **Intercept (β0 =30,000)**: A house with 0 sq ft (theoretically) would cost $30,000 (baseline).\n",
        "\n",
        "* **Slope (β0=120)**: For each additional square foot, the house price increases by $120.\n",
        "\n",
        "**Step 4: Make Predictions**\n",
        "\n",
        "* **Example Prediction:** For a house of 1600 sq ft:\n",
        "\n",
        "##  Price=30,000+120×1600=222,000\n",
        "\n",
        "* This prediction helps buyers, sellers, and real estate agents make data-driven decisions.\n",
        "\n",
        "**Step 5: Benefits of Using SLR in This Example**\n",
        "\n",
        "1. **Prediction:** Estimate prices for new houses based on size.\n",
        "2. **Understanding Relationship:** Quantifies how size affects price.\n",
        "3. **Decision Making:** Helps set fair prices and forecast market trends.\n",
        "4. **Visualization:** Scatter plots and regression line make it easy to interpret trends.\n",
        "\n",
        "**Step 6: Conclusion**\n",
        "\n",
        "Simple Linear Regression is highly useful in real estate, finance, education, healthcare, and many other fields where a dependent variable is influenced by a single predictor. In this example, it provides a straightforward, interpretable model to predict house prices from size, enabling better planning, forecasting, and decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 5: What is the method of least squares in linear regression?**\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "The method of least squares is a fundamental technique used in linear regression to determine the best-fitting line through a set of data points. It ensures that the regression line is positioned such that it minimizes the difference between observed and predicted values.\n",
        "\n",
        "**1. Purpose of the Method of Least Squares**\n",
        "\n",
        "* To find the regression coefficients β0 and β1 in the equation:\n",
        "##  Y = β0 + β1 X + ϵ\n",
        "* To ensure the sum of the squared differences between the observed values (Y i) and the predicted values y pred i is as small as possible.\n",
        "\n",
        "**Mathematical Goal:**\n",
        "Minimize the sum of squared residuals (errors):\n",
        "\n",
        "## S = i=1∑n​(Yi ​− Y^i​)2 = i=1∑n​(Yi ​− (β0​+β1​Xi​))2\n",
        "\n",
        "Where:\n",
        "\n",
        "* (Y_i) = Actual observed value\n",
        "* (\\hat{Y}_i = \\beta_0 + \\beta_1 X_i) = Predicted value\n",
        "* (S) = Sum of squared errors\n",
        "\n",
        "**2. Why Squared Errors?**\n",
        "\n",
        "* Squaring ensures that negative and positive differences do not cancel out.\n",
        "* Squared differences give larger weight to bigger errors, making the line fit the data more accurately.\n",
        "* This approach produces a unique solution for β0 and β1.\n",
        "\n",
        "**3. Formulas for Regression Coefficients**\n",
        "\n",
        "The **slope β1** is calculated as:\n",
        "\n",
        "## β1 =  ∑(Xi − Xˉ)(Yi − Yˉ) / ∑(Xi− Xˉ)2\n",
        "\n",
        "The **intercept β0** is calculated as:\n",
        "\n",
        "## β0 = Yˉ - β1 Xˉ\n",
        "\n",
        "Where:\n",
        "\n",
        "* Xˉ = Mean of X\n",
        "* Yˉ = Mean of Y\n",
        "\n",
        "**4. Steps in the Method of Least Squares**\n",
        "\n",
        "1. **Collect Data:** Gather paired observations of X and Y.\n",
        "2. **Calculate Means:** Compute Xˉ and Yˉ.\n",
        "3. **Compute Slope (β1)** using the formula above.\n",
        "4. **Compute Intercept (β0)**.\n",
        "5. **Form Regression Equation:**\n",
        "   \n",
        "   Y pred = β0 + β1 X\n",
        "   \n",
        "6. **Make Predictions:** Use the equation to predict Y for any value of X.\n",
        "7. **Assess Fit:** Evaluate the model using R-squared, residual plots, or error measures.\n",
        "\n",
        "**5. Example**\n",
        "\n",
        "Consider the following dataset of hours studied (X) and marks obtained (Y):\n",
        "\n",
        "| X (Hours) | Y (Marks) |\n",
        "| --------- | --------- |\n",
        "| 2         | 50        |\n",
        "| 3         | 55        |\n",
        "| 5         | 70        |\n",
        "| 7         | 75        |\n",
        "\n",
        "**Step 1: Compute Means**\n",
        "Xˉ =4.25, Yˉ=62.5\n",
        "\n",
        "**Step 2: Compute Slope (β1)**\n",
        "\n",
        "## β1 = ∑(Xi​−Xˉ)(Yi​−Yˉ) / ∑(Xi​−Xˉ)2 ​=5\n",
        "\n",
        "**Step 3: Compute Intercept (β0)**\n",
        "\n",
        "## β0 = Yˉ− β1 Xˉ= 62.5−(5⋅4.25) = 41.25\n",
        "\n",
        "**Step 4: Regression Equation**\n",
        "\n",
        "## Y pred = 41.25 + 5 X\n",
        "\n",
        "**Step 5: Prediction**\n",
        "\n",
        "* For X = 6 hours:\n",
        "\n",
        "##    Y pred = 41.25+5(6) = 71.25\n",
        "\n",
        "**6. Advantages of Least Squares Method**\n",
        "\n",
        "1. Provides the best-fitting line by minimizing overall errors.\n",
        "2. Produces unique values for slope and intercept.\n",
        "3. Simple and widely applicable in real-world prediction problems.\n",
        "4. Forms the foundation for multiple linear regression and advanced predictive models.\n",
        "\n",
        "**7. Conclusion**\n",
        "\n",
        "The method of least squares is the most commonly used technique to estimate the regression line in simple linear regression. By minimizing the sum of squared differences between observed and predicted values, it ensures the model accurately captures the relationship between X and Y, making it highly useful for prediction, trend analysis, and decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 6: What is Logistic Regression? How does it differ from Linear Regression?**\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "**1. Definition of Logistic Regression**\n",
        "\n",
        "Logistic Regression is a statistical and machine learning technique used to model the relationship between one or more independent variables (predictors) and a categorical dependent variable — most commonly a binary outcome (i.e., having two possible values such as *Yes/No*, *0/1*, *True/False*).\n",
        "\n",
        "Unlike Linear Regression, which predicts continuous numerical outcomes, Logistic Regression predicts the probability that an observation belongs to a particular category.\n",
        "\n",
        "**2. When to Use Logistic Regression**\n",
        "\n",
        "Logistic Regression is used when the dependent variable is categorical, such as:\n",
        "\n",
        "* Predicting whether a student passes or fails an exam.\n",
        "* Predicting whether a customer will buy a product or not.\n",
        "* Predicting if an email is spam or not spam.\n",
        "* Predicting whether a patient has a disease (Yes/No).\n",
        "\n",
        "**3. Mathematical Form of Logistic Regression**\n",
        "\n",
        "In Linear Regression, the model is:\n",
        "\n",
        "## Y=β0​+β1​X+ϵ\n",
        "\n",
        "\n",
        "However, in Logistic Regression, the outcome (Y) is not directly predicted — instead, we predict the probability that (Y = 1) using the logistic (sigmoid) function.\n",
        "\n",
        "The equation is:\n",
        "\n",
        "## P(Y=1∣X)=1​ / 1 + e -(β0​+β1​X)\n",
        "\n",
        "Where:\n",
        "\n",
        "* (P(Y=1|X)) = Probability that Y = 1 for a given X.\n",
        "* e = Base of the natural logarithm (~2.718).\n",
        "* β0​, β1​ = Coefficients determined from data.\n",
        "\n",
        "**4. The Logistic (Sigmoid) Function**\n",
        "\n",
        "The sigmoid curve transforms the linear combination of inputs into a bounded probability between 0 and 1.\n",
        "\n",
        "Sigmoid Function: 1 /f(z)=1 + e - z\n",
        "\n",
        "This means:\n",
        "\n",
        "* If (z) is very large → (f(z)) ≈ 1\n",
        "* If (z) is very small → (f(z)) ≈ 0\n",
        "\n",
        "Thus, Logistic Regression outputs probabilities that can be thresholded (e.g., if P > 0.5 → predict 1, else 0).\n",
        "\n",
        "**5. Example: Predicting Exam Success**\n",
        "\n",
        "Suppose we want to predict whether a student passes (1) or fails (0) based on hours studied (X).\n",
        "The model may look like:\n",
        "\n",
        "P(Pass)=1 / 1+e−(−4+1.2X)\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "* β0 = -4: The log-odds of passing when X = 0.\n",
        "* β1 = 1.2: For each additional hour studied, the odds of passing increase.\n",
        "\n",
        "If a student studies **3 hours**:\n",
        "\n",
        "\n",
        "## p(Pass) =1 / 1+𝑒−(−4+1.2(3)) = 0.77\n",
        "\n",
        "→ There’s a **77% chance** the student will pass.\n",
        "\n",
        "**6. Differences Between Logistic and Linear Regression**\n",
        "\n",
        "| **Feature**                        | **Linear Regression**                     | **Logistic Regression**                          |                                                |\n",
        "| ---------------------------------- | ----------------------------------------- | ------------------------------------------------ | ---------------------------------------------- |\n",
        "| **Type of Dependent Variable**     | Continuous (e.g., salary, temperature)    | Categorical (e.g., yes/no, 0/1)                  |                                                |\n",
        "| **Output Range**                   | Any real number (−∞ to +∞)                | Probability between 0 and 1                      |                                                |\n",
        "| **Equation Form**                  | Y=β0​+β1​X+ϵ      | (P(Y=1))                                          |\n",
        "| **Error Distribution**             | Errors assumed to be normally distributed | Follows a binomial distribution                  |                                                |\n",
        "| **Purpose**                        | Predict a numeric value                   | Predict a class or probability                   |                                                |\n",
        "| **Graph Type**                     | Straight line                             | S-shaped sigmoid curve                           |                                                |\n",
        "| **Loss Function**                  | Mean Squared Error (MSE)                  | Log-Loss or Cross-Entropy                        |                                                |\n",
        "| **Interpretation of Coefficients** | Direct change in Y per unit change in X   | Change in **log-odds** of Y per unit change in X |                                                |\n",
        "\n",
        "**7. Advantages of Logistic Regression**\n",
        "\n",
        "1. Provides probability-based predictions, not just class labels.\n",
        "2. Easy to implement and interpret.\n",
        "3. Works well for binary classification problems.\n",
        "4. Computationally efficient — less complex than other machine learning models.\n",
        "\n",
        "**8. Limitations of Logistic Regression**\n",
        "\n",
        "1. Only suitable for binary or categorical dependent variables.\n",
        "2. Assumes a linear relationship between predictors and log-odds, not between X and Y directly.\n",
        "3. Not ideal for very large, complex datasets with many predictors.\n",
        "4. Sensitive to outliers and multicollinearity.\n",
        "\n",
        "**9. Graphical Interpretation**\n",
        "\n",
        "* The logistic regression curve is S-shaped (sigmoid).\n",
        "* The curve starts near 0 (low probability) and rises toward 1 as X increases.\n",
        "* The threshold (usually 0.5) determines the decision boundary between two classes.\n",
        "\n",
        "**10. Conclusion**\n",
        "\n",
        "Logistic Regression is a powerful statistical tool for classification problems.\n",
        "While Linear Regression models continuous outcomes, Logistic Regression models probabilities of categorical outcomes by transforming linear predictions using the sigmoid function.\n",
        "\n",
        "It is widely used in medicine (disease diagnosis), finance (loan approval), marketing (customer churn), and social sciences for predictive analysis and decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 7: Name and briefly describe three common evaluation metrics for regression models.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "After building a regression model, it’s important to evaluate how well the model performs in predicting outcomes. Evaluation metrics measure the accuracy and reliability of the model’s predictions compared to the actual observed data.\n",
        "\n",
        "For regression models, which predict continuous values, common metrics include:\n",
        "\n",
        "1. **Mean Absolute Error (MAE)**\n",
        "2. **Mean Squared Error (MSE)**\n",
        "3. **R-squared (Coefficient of Determination)**\n",
        "\n",
        "These metrics help assess how close predicted values are to actual values and indicate how well the model fits the data.\n",
        "\n",
        "**1. Mean Absolute Error (MAE)**\n",
        "\n",
        "**Definition:**\n",
        "The Mean Absolute Error measures the average magnitude of errors in a set of predictions, without considering their direction (positive or negative).\n",
        "\n",
        "It represents the average absolute difference between predicted and actual values.\n",
        "\n",
        "**Formula:**\n",
        "## MAE=1/n ​i=1∑n ​∣Yi​−Yi​^​∣\n",
        "\n",
        "Where:\n",
        "\n",
        "* Yi = actual value\n",
        "* Yi​^ = predicted value\n",
        "* n = number of observations\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "* MAE = 0 → perfect prediction\n",
        "* Lower MAE indicates better model performance.\n",
        "\n",
        "**Example:**\n",
        "If actual prices are [100, 120, 150] and predicted prices are [110, 125, 140],\n",
        "\n",
        "MAE = |100-110| + |120-125| + |150-140| / 3 = 10 + 5 + 10 / 3 = 8.33\n",
        "\n",
        "Thus, the average prediction error is ₹8.33.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Simple and easy to interpret.\n",
        "* Less sensitive to outliers than MSE.\n",
        "\n",
        "**2. Mean Squared Error (MSE)**\n",
        "\n",
        "**Definition:**\n",
        "The Mean Squared Error calculates the average of squared differences between actual and predicted values.\n",
        "Squaring the errors ensures that large errors are penalized more heavily.\n",
        "\n",
        "**Formula:**\n",
        "## MSE = 1 / n ​i=1∑n ​(Yi​−Yi​^​)2\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "* MSE = 0 → perfect fit.\n",
        "* The lower the MSE, the better the model.\n",
        "* Since the errors are squared, MSE emphasizes larger errors.\n",
        "\n",
        "**Example:**\n",
        "Using the same data:\n",
        "\n",
        "MSE = (100-110)^2 + (120-125)^2 + (150-140)^2 / 3 = 100 + 25 + 100 / 3 = 75\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Highlights large prediction errors.\n",
        "* Commonly used in model optimization (e.g., gradient descent minimizes MSE).\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* Because errors are squared, MSE is sensitive to outliers.\n",
        "\n",
        "**3. R-Squared (Coefficient of Determination)**\n",
        "\n",
        "**Definition:**\n",
        "The R-squared (R²) value measures the proportion of variance in the dependent variable that is explained by the regression model.\n",
        "\n",
        "It shows how well the model fits the data.\n",
        "\n",
        "**Formula:**\n",
        "R2= 1 − ​SSres​​ / SStot\n",
        "\n",
        "Where:\n",
        "\n",
        "* SSres=∑(Yi−Yi^)2 → Residual Sum of Squares\n",
        "* SStot=∑(Yi−Yˉ)2 → Total Sum of Squares\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "* R² = 1: Perfect fit (all points lie on the regression line).\n",
        "* R² = 0: Model explains none of the variability.\n",
        "* Higher R² means better fit.\n",
        "\n",
        "**Example:**\n",
        "If R² = 0.85, it means 85% of the variation in the dependent variable is explained by the model.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Gives a clear measure of model performance.\n",
        "* Useful for comparing multiple regression models.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "* Cannot detect overfitting.\n",
        "* Can only increase when more variables are added, even if they are irrelevant.\n",
        "\n",
        "**Summary Table**\n",
        "\n",
        "| **Metric** | **Formula**                      | **Range**   | **Goal** | **Interpretation**                        |          |                                   |\n",
        "| ---------- | -------------------------------- | ----------- | -------- | ----------------------------------------- | -------- | --------------------------------- |\n",
        "| **MAE**    | MAE=1/n ​n∑i=1​∣Yi​−Yi​^​∣ | ≥ 0 | Minimize | Average absolute prediction error |\n",
        "| **MSE**    | 1/n​∑(Y−Y^)2 | ≥ 0         | Minimize | Penalizes large errors more               |          |                                   |\n",
        "| **R²**     | 1− SSres / SStot​​​  | 0 to 1      | Maximize | Proportion of variance explained by model |          |                                   |\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "The evaluation of a regression model is essential for assessing its predictive accuracy and reliability.\n",
        "\n",
        "* **MAE** provides an intuitive measure of average error,\n",
        "* **MSE** emphasizes large deviations, and\n",
        "* **R²** explains how well the model fits the data.\n",
        "\n",
        "In practice, analysts often use all three together to gain a comprehensive understanding of model performance before final deployment.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 8: What is the purpose of the R-squared metric in regression analysis?**\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "**1. Introduction**\n",
        "\n",
        "In regression analysis, it is important to understand how well the model explains the variation in the dependent variable.\n",
        "The R-squared (R²) metric — also known as the coefficient of determination — serves this exact purpose.\n",
        "\n",
        "It measures the proportion of variance in the dependent variable that can be explained by the independent variable(s) in the model.\n",
        "\n",
        "**2. Definition of R-squared**\n",
        "\n",
        "R-squared (R²) is a statistical measure that shows how well the regression predictions approximate the real data points.\n",
        "\n",
        "It indicates the goodness of fit — how well the regression line represents the actual data.\n",
        "\n",
        "R² = 1 - SSres / SStot\n",
        "\n",
        "Where:\n",
        "\n",
        "* SSres = ∑(Yi - Yi)^2 → Residual Sum of Squares (unexplained variation)\n",
        "* SStot = ∑(Yi - Yˉ)^2 → Total Sum of Squares (total variation)\n",
        "* Yi: actual value\n",
        "* Yi​^: predicted value\n",
        "* Yˉ: mean of actual values\n",
        "\n",
        "**3. Interpretation of R-squared**\n",
        "\n",
        "* ( R^2 ) values range from 0 to 1.\n",
        "* A higher R² value means the model explains more variation in the data.\n",
        "\n",
        "| **R² Value**   | **Interpretation**                                              |\n",
        "| -------------- | --------------------------------------------------------------- |\n",
        "| 0              | Model explains none of the variability in Y                     |\n",
        "| 0 < R² < 0.5   | Weak fit; model explains little of the variation                |\n",
        "| 0.5 ≤ R² < 0.8 | Moderate fit; model explains a fair amount of variation         |\n",
        "| 0.8 ≤ R² ≤ 1   | Strong fit; model explains most of the variation                |\n",
        "| 1              | Perfect fit; all data points lie exactly on the regression line |\n",
        "\n",
        "**4. Purpose of R-squared**\n",
        "\n",
        "The main purposes of using the R² metric in regression analysis are:\n",
        "\n",
        "**(a) Measure of Goodness of Fit**\n",
        "\n",
        "R² indicates how well the regression line fits the data points.\n",
        "\n",
        "* A higher R² means predictions are closer to actual values.\n",
        "* A lower R² suggests the model may not represent the data well.\n",
        "\n",
        "**(b) Explains Variability**\n",
        "\n",
        "It tells what percentage of the total variation in the dependent variable is explained by the independent variable(s).\n",
        "For example, if R² = 0.85, then 85% of the variation in Y is explained by the model, and 15% remains unexplained.\n",
        "\n",
        "**(c) Model Comparison**\n",
        "\n",
        "R² allows for comparing the performance of different regression models.\n",
        "\n",
        "* A model with a higher R² generally provides a better fit.\n",
        "* However, it should be compared with caution, as R² alone does not detect overfitting.\n",
        "\n",
        "**(d) Model Improvement Indicator**\n",
        "\n",
        "Adding more relevant independent variables should increase R² if they help explain additional variation in Y.\n",
        "Thus, R² helps determine whether new variables improve or weaken the model.\n",
        "\n",
        "**5. Example**\n",
        "\n",
        "Suppose we are predicting students’ exam scores (Y) based on hours studied (X.\n",
        "\n",
        "| Student | Hours Studied (X) | Actual Score (Y) | Predicted Score (Ŷ) |\n",
        "| ------- | ----------------- | ---------------- | ------------------- |\n",
        "| 1       | 2                 | 40               | 45                  |\n",
        "| 2       | 4                 | 55               | 50                  |\n",
        "| 3       | 6                 | 65               | 60                  |\n",
        "| 4       | 8                 | 80               | 75                  |\n",
        "\n",
        "Now:\n",
        "\n",
        "SSres = ∑(Yi - Yi)^2 = (40-45)^2 + (55-50)^2 + (65-60)^2 + (80-75)^2 = 25 + 25 + 25 + 25 = 100\n",
        "\n",
        "SStot = ∑(Yi - Yˉ)^2 = (40-60)^2 + (55-60)^2 + (65-60)^2 + (80-60)^2 = 400 + 25 + 25 + 400 = 850\n",
        "\n",
        "R^2 = 1 - SSres / SStot = 1 - 100 / 850 = 1 - 0.118 = 0.882\n",
        "\n",
        "\n",
        "So, R² = 0.88, meaning the model explains 88% of the variation in exam scores.\n",
        "\n",
        "**6. Advantages of R-squared**\n",
        "\n",
        "1. **Simple and Intuitive:** Easy to calculate and interpret.\n",
        "2. **Useful for Model Comparison:** Quickly identifies which model fits data better.\n",
        "3. **Explains Variability Clearly:** Provides a clear percentage of variation explained by the model.\n",
        "\n",
        "**7. Limitations of R-squared**\n",
        "\n",
        "1. **Does Not Indicate Causation:** A high R² does not mean that X causes Y.\n",
        "2. **Cannot Detect Overfitting:** Adding more variables can artificially increase R² even if they’re irrelevant.\n",
        "3. **No Measure of Bias or Accuracy:** A model can have a high R² but still perform poorly on unseen data.\n",
        "4. **Not Suitable Alone for Nonlinear Models:** R² mainly evaluates linear relationships.\n",
        "\n",
        "To handle overfitting, analysts often use Adjusted R-squared, which adjusts R² based on the number of predictors and sample size.\n",
        "\n",
        "## Adjusted R² = 1 − (1−R2)(n−1) / (n−k−1​)\n",
        "\n",
        "where *n* = number of observations, *k* = number of predictors.\n",
        "\n",
        "**8. Conclusion**\n",
        "\n",
        "The R-squared metric is a crucial tool in regression analysis as it measures how well the model fits the data.\n",
        "It explains the percentage of variation in the dependent variable that is captured by the independent variable(s).\n",
        "\n",
        "While a high R² indicates a strong fit, it should be used alongside other metrics (like MAE or MSE) to fully assess the model’s performance and avoid overfitting.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "izL4gQwQcbEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "apeDARpynFHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# Simple Linear Regression using scikit-learn\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# Step 1: Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Step 2: Prepare the data\n",
        "# X represents the independent variable (e.g., hours studied)\n",
        "# y represents the dependent variable (e.g., exam score)\n",
        "X = np.array([[1], [2], [3], [4], [5]])   # Independent variable\n",
        "y = np.array([2, 4, 5, 4, 5])              # Dependent variable\n",
        "\n",
        "# Step 3: Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Step 4: Fit the model to the data\n",
        "# The model learns the relationship between X and y\n",
        "model.fit(X, y)\n",
        "\n",
        "# Step 5: Print the slope (coefficient) and intercept\n",
        "# The slope shows how much y changes for a unit change in X\n",
        "# The intercept is the predicted value of y when X = 0\n",
        "print(\"Slope (Coefficient):\", model.coef_[0])\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "# Step 6: Predict values using the trained model (optional)\n",
        "# This predicts y-values for each value in X\n",
        "predictions = model.predict(X)\n",
        "print(\"Predicted Values:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHCGW5YDnEX-",
        "outputId": "0db722aa-8062-44d9-948d-c2ff461c5bf9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (Coefficient): 0.6\n",
            "Intercept: 2.2\n",
            "Predicted Values: [2.8 3.4 4.  4.6 5.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 10: How do you interpret the coefficients in a simple linear regression model?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "In a Simple Linear Regression (SLR) model, the goal is to establish a linear relationship between one independent variable (X) and one dependent variable (Y).\n",
        "The general equation of a simple linear regression model is:\n",
        "\n",
        "## Y=β0​+β1​X+ϵ\n",
        "\n",
        "Where:\n",
        "\n",
        "* Y → Dependent variable (what we want to predict)\n",
        "* X → Independent variable (predictor)\n",
        "* β0​ → Intercept or constant term\n",
        "* β1​ → Slope or regression coefficient\n",
        "* ϵ → Random error term\n",
        "\n",
        "Each coefficient in this model has a specific interpretation related to the relationship between X and Y.\n",
        "\n",
        "**1. Intercept β0**\n",
        "\n",
        "* The intercept represents the predicted value of Y when X = 0.\n",
        "* It is the point where the regression line crosses the Y-axis.\n",
        "* In practical terms, it gives the baseline or starting value of the dependent variable before the independent variable has any effect.\n",
        "\n",
        "**Example:**\n",
        "If the regression equation is:\n",
        "## Y^=25+4X\n",
        "then the intercept (β0 = 25) means that when (X = 0), the predicted value of (Y) is 25.\n",
        "If Y represents sales and X represents advertising expense, it means that even with zero advertising, the business expects 25 units of sales (possibly due to brand reputation or repeat customers).\n",
        "\n",
        "**2. Slope β1**\n",
        "\n",
        "* The slope shows how much Y changes for a one-unit increase in X.\n",
        "* It indicates both the direction and magnitude of the relationship.\n",
        "* A positive slope means Y increases as X increases.\n",
        "* A negative slope means Y decreases as X increases.\n",
        "* A slope close to zero means there is little or no linear relationship.\n",
        "\n",
        "**Example:**\n",
        "From the same equation:\n",
        "Y^=25+4X\n",
        "Here, β1 = 4 means that for every 1-unit increase in X, Y increases by 4 units.\n",
        "So, for every extra ₹1000 spent on advertising, the sales are expected to rise by ₹4000.\n",
        "\n",
        "**3. Combined Interpretation**\n",
        "\n",
        "The regression equation combines both coefficients to predict Y for any value of X.\n",
        "In the above example:\n",
        "## Y^=25+4X\n",
        "\n",
        "* The **intercept (25)** gives the base value of Y.\n",
        "* The **slope (4)** shows how much Y changes when X increases by 1 unit.\n",
        "\n",
        "Thus, these coefficients together define the best-fit line that represents the average relationship between X and Y.\n",
        "\n",
        "**4. Visualization Insight**\n",
        "\n",
        "On a graph:\n",
        "\n",
        "* The slope determines the tilt or steepness of the regression line.\n",
        "* The intercept determines where the line crosses the Y-axis.\n",
        "\n",
        "Together, they form the linear model used for prediction.\n",
        "\n",
        "**5. Important Notes**\n",
        "\n",
        "1. The interpretation assumes a linear relationship between X and Y.\n",
        "2. The intercept may not always have a practical meaning (for example, when X = 0 is not possible).\n",
        "3. Outliers or unusual data points can affect both coefficients.\n",
        "4. The coefficients are estimated using the method of least squares, which minimizes the sum of squared errors between predicted and actual values.\n",
        "\n",
        "**6. Example in Words**\n",
        "\n",
        "If a regression equation predicting student marks (Y) from study hours (X) is:\n",
        "## Y^=20+5X\n",
        "\n",
        "* Intercept (20): When a student studies 0 hours, they are expected to score 20 marks.\n",
        "* Slope (5): For every additional hour of study, the student’s marks increase by 5 marks.\n",
        "\n",
        "**7. Conclusion**\n",
        "\n",
        "In summary:\n",
        "\n",
        "* **Intercept β0 ** → Predicted value of Y when X = 0.\n",
        "* **Slope β1​ ** → Change in Y for every 1-unit increase in X.\n",
        "\n",
        "These coefficients together describe how the dependent variable responds to changes in the independent variable, allowing predictions and understanding of trends in data.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "sJggydW-nPKk"
      }
    }
  ]
}